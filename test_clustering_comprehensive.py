#!/usr/bin/env python3
"""
Comprehensive TDD Test Suite for Newsletter Clustering System

Following London School TDD approach:
1. Write failing tests first
2. Implement minimal code to pass
3. Refactor after green

Test Coverage Areas:
- Unit tests for individual components
- Integration tests for workflow
- Performance tests for constraints
- Quality tests for clustering metrics
- Fallback tests for error scenarios
"""

import unittest
import time
import sys
import os
import tempfile
import shutil
from unittest.mock import Mock, patch, MagicMock
from typing import List, Dict, Any, Optional
import numpy as np

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Test imports - these should fail initially following TDD
try:
    from newsletter_agent_core.clustering import (
        ClusteringOrchestrator, ClusteringConfig
    )
    from newsletter_agent_core.clustering.embedding.service import (
        EmbeddingService, EmbeddingCache
    )
    from newsletter_agent_core.clustering.algorithms.hdbscan_clusterer import (
        HDBSCANClusterer, ClusteringResult
    )
    from newsletter_agent_core.clustering.algorithms.hierarchical_clusterer import (
        HierarchicalClusterer
    )
    from newsletter_agent_core.clustering.algorithms.hybrid_clusterer import (
        HybridClusterer
    )
    from newsletter_agent_core.clustering.validation.validator import (
        ClusteringValidator, ValidationResult
    )
    from newsletter_agent_core.clustering.config.settings import (
        EmbeddingConfig, CacheConfig, ValidationConfig, HDBSCANConfig,
        HierarchicalConfig, HybridConfig, PerformanceConfig
    )
    CLUSTERING_AVAILABLE = True
except ImportError as e:
    print(f"WARNING: Clustering components not fully available: {e}")
    CLUSTERING_AVAILABLE = False
    
    # Create mock classes for testing when components aren't available
    class MockConfig:
        def __init__(self):
            pass
    
    class MockPerformanceConfig:
        def __init__(self):
            self.max_processing_time_seconds = 30
            self.max_memory_usage_gb = 2.0
            self.enable_progress_tracking = False
    
    class MockValidationConfig:
        def __init__(self):
            self.min_silhouette_score = 0.3
            self.max_noise_ratio = 0.3
            self.min_cluster_size = 3
    
    # Assign mock classes
    ClusteringConfig = MockConfig
    EmbeddingConfig = MockConfig
    CacheConfig = MockConfig
    ValidationConfig = MockValidationConfig
    HDBSCANConfig = MockConfig
    HierarchicalConfig = MockConfig
    HybridConfig = MockConfig
    PerformanceConfig = MockPerformanceConfig
    
    # Mock other classes
    ClusteringOrchestrator = Mock
    EmbeddingService = Mock
    EmbeddingCache = Mock
    HDBSCANClusterer = Mock
    HierarchicalClusterer = Mock
    HybridClusterer = Mock
    ClusteringValidator = Mock
    ClusteringResult = Mock


class TestDataGenerator:
    """Generate realistic test data for newsletter clustering tests."""
    
    @staticmethod
    def create_newsletter_items(count: int = 50, diverse_topics: bool = True) -> List[Dict[str, Any]]:
        """
        Generate realistic newsletter items for testing.
        
        Args:
            count: Number of items to generate
            diverse_topics: Whether to include diverse topics for clustering
            
        Returns:
            List of newsletter items with realistic content
        """
        # Base templates for different topics
        ai_templates = [
            {
                "master_headline": "AI Breakthrough in {domain}",
                "headline": "{company} announces major AI advancement in {domain}",
                "short_description": "Revolutionary AI technology from {company} promises to transform {domain} with advanced machine learning capabilities and improved efficiency.",
                "source": "{company} Research",
                "companies": ["{company}"],
                "technologies": ["AI", "Machine Learning", "{domain}"],
            },
            {
                "master_headline": "Large Language Model Update",
                "headline": "{company} releases new LLM with enhanced capabilities",
                "short_description": "The latest large language model from {company} features improved reasoning, reduced hallucinations, and better performance on complex tasks.",
                "source": "{company} Blog",
                "companies": ["{company}"],
                "technologies": ["LLM", "Natural Language Processing", "AI"],
            }
        ]
        
        tech_templates = [
            {
                "master_headline": "Cloud Computing Innovation",
                "headline": "{company} launches new cloud service for {domain}",
                "short_description": "New cloud infrastructure from {company} offers scalable solutions for {domain} with improved performance and cost efficiency.",
                "source": "{company} News",
                "companies": ["{company}"],
                "technologies": ["Cloud Computing", "{domain}", "Infrastructure"],
            },
            {
                "master_headline": "Cybersecurity Enhancement",
                "headline": "{company} introduces advanced security measures",
                "short_description": "Enhanced cybersecurity solutions from {company} provide better protection against emerging threats and vulnerabilities.",
                "source": "Security Weekly",
                "companies": ["{company}"],
                "technologies": ["Cybersecurity", "Threat Detection", "Security"],
            }
        ]
        
        business_templates = [
            {
                "master_headline": "Market Expansion Strategy",
                "headline": "{company} expands operations to European markets",
                "short_description": "Strategic expansion by {company} into European markets aims to capture new opportunities and increase market share.",
                "source": "Business Times",
                "companies": ["{company}"],
                "technologies": ["Market Strategy", "Business Development"],
            }
        ]
        
        # Company and domain variations
        companies = ["OpenAI", "Google", "Microsoft", "Amazon", "Meta", "Anthropic", "Salesforce", "IBM", "NVIDIA", "Apple"]
        domains = ["Healthcare", "Finance", "Education", "Retail", "Manufacturing", "Transportation", "Energy", "Media"]
        
        items = []
        template_sets = [ai_templates, tech_templates, business_templates] if diverse_topics else [ai_templates]
        
        for i in range(count):
            # Select template set and template
            template_set = template_sets[i % len(template_sets)]
            template = template_set[i % len(template_set)]
            
            # Select company and domain
            company = companies[i % len(companies)]
            domain = domains[i % len(domains)]
            
            # Create item from template
            item = {}
            for key, value in template.items():
                if isinstance(value, str):
                    item[key] = value.format(company=company, domain=domain)
                elif isinstance(value, list):
                    item[key] = [v.format(company=company, domain=domain) if isinstance(v, str) else v for v in value]
                else:
                    item[key] = value
            
            # Add unique identifiers and dates
            item["id"] = f"test_item_{i:04d}"
            item["date"] = f"2024-01-{(i % 28) + 1:02d}"
            
            # Add variation for larger datasets
            if i >= len(template_sets) * len(companies):
                variation = i // (len(template_sets) * len(companies))
                item["headline"] += f" (Update {variation})"
                item["short_description"] += f" This represents update {variation} with additional context and developments."
            
            items.append(item)
        
        return items
    
    @staticmethod
    def create_edge_case_items() -> List[Dict[str, Any]]:
        """Generate edge case items for testing robustness."""
        return [
            # Empty/minimal content
            {"headline": "", "short_description": "", "source": "Empty Source"},
            {"headline": "A", "short_description": "B", "source": "Minimal"},
            
            # Very long content
            {
                "headline": "Very Long Headline " * 20,
                "short_description": "This is an extremely long description that goes on and on with lots of repetitive content. " * 50,
                "source": "Long Content Source"
            },
            
            # Special characters and encoding
            {
                "headline": "Special Characters: Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«",
                "short_description": "Content with Ã©mojis ðŸš€ðŸ¤– and special chars: @#$%^&*()",
                "source": "Unicode Source"
            },
            
            # Missing fields
            {"headline": "Missing Description"},
            {"short_description": "Missing Headline"},
            {},
            
            # Duplicate content
            {"headline": "Duplicate", "short_description": "Exact same content", "source": "Source A"},
            {"headline": "Duplicate", "short_description": "Exact same content", "source": "Source B"},
        ]
    
    @staticmethod
    def create_performance_test_items(size: int) -> List[Dict[str, Any]]:
        """Generate items specifically for performance testing."""
        # Create realistic but varied content for performance testing
        base_items = TestDataGenerator.create_newsletter_items(min(size, 100), diverse_topics=True)
        
        if size <= 100:
            return base_items[:size]
        
        # For larger sizes, duplicate and vary the base items
        items = []
        for i in range(size):
            base_item = base_items[i % len(base_items)].copy()
            
            # Add variation to avoid exact duplicates
            variation_id = i // len(base_items)
            if variation_id > 0:
                base_item["headline"] += f" - Variation {variation_id}"
                base_item["short_description"] += f" Additional context for variation {variation_id} with more detailed information."
                base_item["id"] = f"perf_item_{i:06d}"
            
            items.append(base_item)
        
        return items


class TestEmbeddingService(unittest.TestCase):
    """Unit tests for EmbeddingService component."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = EmbeddingConfig()
        self.cache_config = CacheConfig()
        self.cache_config.cache_dir = self.temp_dir
    
    def tearDown(self):
        """Clean up test fixtures."""
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_embedding_service_initialization(self):
        """Test that EmbeddingService initializes correctly."""
        service = EmbeddingService(self.config, self.cache_config)
        self.assertIsNotNone(service)
        self.assertEqual(service.config, self.config)
        self.assertEqual(service.cache_config, self.cache_config)
    
    @unittest.skipIf(not CLUSTERING_AVAILABLE, "Clustering not available")
    def test_generate_embeddings_with_valid_texts(self):
        """Test embedding generation with valid input texts."""
        service = EmbeddingService(self.config, self.cache_config)
        texts = ["AI technology advances", "Machine learning breakthrough"]
        
        # Should raise ImportError if sentence-transformers not available
        with self.assertRaises(ImportError):
            embeddings = service.generate_embeddings(texts)
    
    def test_generate_embeddings_with_empty_input(self):
        """Test embedding generation handles empty input gracefully."""
        service = EmbeddingService(self.config, self.cache_config)
        
        # Should raise ValueError for empty input
        with self.assertRaises(ValueError):
            service.generate_embeddings([])
    
    def test_embedding_cache_functionality(self):
        """Test that embedding caching works correctly."""
        cache = EmbeddingCache(self.cache_config, self.temp_dir)
        texts = ["Test text for caching"]
        model_name = "test-model"
        
        # Should return None for cache miss
        result = cache.get(texts, model_name)
        self.assertIsNone(result)
    
    def test_embedding_service_performance_constraints(self):
        """Test that embedding service meets performance requirements."""
        service = EmbeddingService(self.config, self.cache_config)
        texts = TestDataGenerator.create_performance_test_items(100)
        
        start_time = time.time()
        with self.assertRaises(NotImplementedError):
            embeddings = service.generate_embeddings([item["short_description"] for item in texts])
        
        # Should complete within reasonable time
        elapsed = time.time() - start_time
        self.assertLess(elapsed, 10.0, "Embedding generation took too long")


class TestHDBSCANClusterer(unittest.TestCase):
    """Unit tests for HDBSCAN clustering algorithm."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.config = HDBSCANConfig()
        self.clusterer = HDBSCANClusterer(self.config)
    
    def test_hdbscan_clusterer_initialization(self):
        """Test HDBSCAN clusterer initializes correctly."""
        self.assertIsNotNone(self.clusterer)
        self.assertEqual(self.clusterer.config, self.config)
    
    def test_fit_predict_with_valid_embeddings(self):
        """Test HDBSCAN clustering with valid embeddings."""
        # Create mock embeddings
        embeddings = np.random.rand(50, 384)  # 50 items, 384-dim embeddings
        
        with self.assertRaises(NotImplementedError):
            result = self.clusterer.fit_predict(embeddings)
    
    def test_fit_predict_with_insufficient_data(self):
        """Test HDBSCAN handles insufficient data gracefully."""
        # Too few items for clustering
        embeddings = np.random.rand(2, 384)
        
        with self.assertRaises(ValueError):
            result = self.clusterer.fit_predict(embeddings)
    
    def test_clustering_result_structure(self):
        """Test that clustering result has expected structure."""
        embeddings = np.random.rand(50, 384)
        
        with self.assertRaises(NotImplementedError):
            result = self.clusterer.fit_predict(embeddings)
            
        # Should have required attributes
        # self.assertIsInstance(result, ClusteringResult)
        # self.assertTrue(hasattr(result, 'labels'))
        # self.assertTrue(hasattr(result, 'n_clusters'))
        # self.assertTrue(hasattr(result, 'n_noise'))


class TestHierarchicalClusterer(unittest.TestCase):
    """Unit tests for Hierarchical clustering algorithm."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.config = HierarchicalConfig()
        self.clusterer = HierarchicalClusterer(self.config)
    
    def test_hierarchical_clusterer_initialization(self):
        """Test Hierarchical clusterer initializes correctly."""
        self.assertIsNotNone(self.clusterer)
        self.assertEqual(self.clusterer.config, self.config)
    
    def test_fit_predict_with_valid_embeddings(self):
        """Test Hierarchical clustering with valid embeddings."""
        embeddings = np.random.rand(50, 384)
        
        with self.assertRaises(NotImplementedError):
            result = self.clusterer.fit_predict(embeddings)
    
    def test_different_linkage_methods(self):
        """Test different linkage methods for hierarchical clustering."""
        embeddings = np.random.rand(30, 384)
        
        for linkage in ['ward', 'complete', 'average']:
            self.config.linkage = linkage
            clusterer = HierarchicalClusterer(self.config)
            
            with self.assertRaises(NotImplementedError):
                result = clusterer.fit_predict(embeddings)


class TestHybridClusterer(unittest.TestCase):
    """Unit tests for Hybrid clustering algorithm."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.hybrid_config = HybridConfig()
        self.hdbscan_config = HDBSCANConfig()
        self.hierarchical_config = HierarchicalConfig()
        self.clusterer = HybridClusterer(
            self.hybrid_config, 
            self.hdbscan_config, 
            self.hierarchical_config
        )
    
    def test_hybrid_clusterer_initialization(self):
        """Test Hybrid clusterer initializes correctly."""
        self.assertIsNotNone(self.clusterer)
    
    def test_fit_predict_combines_algorithms(self):
        """Test that hybrid approach combines multiple algorithms."""
        embeddings = np.random.rand(100, 384)
        
        with self.assertRaises(NotImplementedError):
            result = self.clusterer.fit_predict(embeddings)
    
    def test_hybrid_fallback_mechanism(self):
        """Test that hybrid clustering falls back appropriately."""
        # Test with data that might cause one algorithm to fail
        embeddings = np.random.rand(5, 384)  # Very small dataset
        
        with self.assertRaises(NotImplementedError):
            result = self.clusterer.fit_predict(embeddings)


class TestClusteringValidator(unittest.TestCase):
    """Unit tests for clustering validation component."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.config = ValidationConfig()
        self.validator = ClusteringValidator(self.config)
    
    def test_validator_initialization(self):
        """Test validator initializes correctly."""
        self.assertIsNotNone(self.validator)
        self.assertEqual(self.validator.config, self.config)
    
    def test_validate_clustering_with_good_results(self):
        """Test validation with high-quality clustering results."""
        embeddings = np.random.rand(50, 384)
        
        # Mock clustering result
        clustering_result = Mock(spec=ClusteringResult)
        clustering_result.labels = np.array([0, 0, 1, 1, 2, 2] * 8 + [0, 0])  # 50 items
        clustering_result.n_clusters = 3
        clustering_result.n_noise = 0
        clustering_result.algorithm = "test"
        clustering_result.processing_time = 1.0
        
        with self.assertRaises(NotImplementedError):
            result = self.validator.validate_clustering(embeddings, clustering_result)
    
    def test_validate_clustering_with_poor_results(self):
        """Test validation identifies poor clustering results."""
        embeddings = np.random.rand(50, 384)
        
        # Mock poor clustering result (all noise)
        clustering_result = Mock(spec=ClusteringResult)
        clustering_result.labels = np.array([-1] * 50)  # All noise
        clustering_result.n_clusters = 0
        clustering_result.n_noise = 50
        clustering_result.algorithm = "test"
        clustering_result.processing_time = 1.0
        
        with self.assertRaises(NotImplementedError):
            result = self.validator.validate_clustering(embeddings, clustering_result)
    
    def test_diversity_metrics_calculation(self):
        """Test that diversity metrics are calculated correctly."""
        items = TestDataGenerator.create_newsletter_items(30, diverse_topics=True)
        embeddings = np.random.rand(30, 384)
        
        clustering_result = Mock(spec=ClusteringResult)
        clustering_result.labels = np.array([0, 0, 1, 1, 2, 2] * 5)
        clustering_result.n_clusters = 3
        clustering_result.n_noise = 0
        clustering_result.algorithm = "test"
        clustering_result.processing_time = 1.0
        
        with self.assertRaises(NotImplementedError):
            result = self.validator.validate_clustering(embeddings, clustering_result, items)


class TestClusteringOrchestrator(unittest.TestCase):
    """Integration tests for the main clustering orchestrator."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.config = ClusteringConfig()
        self.temp_dir = tempfile.mkdtemp()
        self.config.cache.cache_dir = self.temp_dir
    
    def tearDown(self):
        """Clean up test fixtures."""
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_orchestrator_initialization(self):
        """Test orchestrator initializes all components correctly."""
        with self.assertRaises(NotImplementedError):
            orchestrator = ClusteringOrchestrator(self.config)
    
    def test_cluster_items_end_to_end_workflow(self):
        """Test complete clustering workflow from items to results."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_newsletter_items(50)
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(
                items=items,
                text_field="short_description",
                algorithm="hdbscan",
                validate_results=True
            )
    
    def test_cluster_items_with_empty_input(self):
        """Test orchestrator handles empty input gracefully."""
        orchestrator = ClusteringOrchestrator(self.config)
        
        result = orchestrator.cluster_items(items=[])
        
        # Should return empty result structure
        self.assertEqual(result["total_items"], 0)
        self.assertEqual(result["total_clusters"], 0)
        self.assertFalse(result["is_valid"])
    
    def test_cluster_items_with_different_algorithms(self):
        """Test orchestrator works with all supported algorithms."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_newsletter_items(30)
        
        algorithms = ["hdbscan", "hierarchical", "hybrid"]
        
        for algorithm in algorithms:
            with self.subTest(algorithm=algorithm):
                with self.assertRaises(NotImplementedError):
                    result = orchestrator.cluster_items(
                        items=items,
                        algorithm=algorithm,
                        validate_results=False
                    )


class TestPerformanceConstraints(unittest.TestCase):
    """Performance tests to validate time and memory constraints."""
    
    def setUp(self):
        """Set up performance test configuration."""
        self.config = ClusteringConfig()
        self.config.performance.max_processing_time_seconds = 30
        self.config.performance.max_memory_usage_gb = 2.0
    
    def test_performance_with_400_items(self):
        """Test performance meets requirements with 400 items."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_performance_test_items(400)
        
        start_time = time.time()
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(
                items=items,
                text_field="short_description",
                algorithm="hybrid",
                validate_results=True
            )
        
        elapsed_time = time.time() - start_time
        
        # Should meet performance requirements
        self.assertLess(elapsed_time, 30.0, "Processing time exceeded 30 seconds")
        
        # Check memory usage if available in result
        # if "performance_metrics" in result:
        #     memory_gb = result["performance_metrics"]["peak_memory_gb"]
        #     self.assertLess(memory_gb, 2.0, "Memory usage exceeded 2GB")
    
    def test_performance_scaling(self):
        """Test performance scales appropriately with input size."""
        orchestrator = ClusteringOrchestrator(self.config)
        
        sizes = [50, 100, 200]
        times = []
        
        for size in sizes:
            items = TestDataGenerator.create_performance_test_items(size)
            
            start_time = time.time()
            with self.assertRaises(NotImplementedError):
                result = orchestrator.cluster_items(items=items, validate_results=False)
            elapsed = time.time() - start_time
            times.append(elapsed)
        
        # Performance should scale reasonably (not exponentially)
        # This is a basic check - times[2] shouldn't be more than 4x times[0]
        if len(times) == 3:
            self.assertLess(times[2], times[0] * 4, "Performance scaling is poor")


class TestClusteringQuality(unittest.TestCase):
    """Tests for clustering quality metrics and validation."""
    
    def setUp(self):
        """Set up quality test configuration."""
        self.config = ClusteringConfig()
        self.config.validation.min_silhouette_score = 0.3
        self.config.validation.max_noise_ratio = 0.3
        self.config.validation.min_cluster_size = 3
    
    def test_clustering_quality_metrics(self):
        """Test that clustering quality meets minimum thresholds."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_newsletter_items(100, diverse_topics=True)
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(
                items=items,
                algorithm="hybrid",
                validate_results=True
            )
        
        # Should meet quality thresholds
        # self.assertTrue(result["is_valid"], "Clustering should be valid")
        # self.assertGreaterEqual(result["quality_score"], 0.5, "Quality score too low")
        
        # Check noise ratio
        # noise_ratio = result["noise_items"] / result["total_items"]
        # self.assertLessEqual(noise_ratio, 0.3, "Too many noise items")
    
    def test_source_diversity_in_clusters(self):
        """Test that clusters maintain source diversity."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_newsletter_items(80, diverse_topics=True)
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(items=items, validate_results=True)
        
        # Check cluster summaries for source diversity
        # for cluster in result["cluster_summaries"]:
        #     if cluster["size"] >= 5:  # Only check larger clusters
        #         sources = cluster["top_sources"]
        #         self.assertGreaterEqual(len(sources), 2, "Cluster lacks source diversity")


class TestErrorHandlingAndFallbacks(unittest.TestCase):
    """Tests for error handling and graceful fallback scenarios."""
    
    def setUp(self):
        """Set up error handling test configuration."""
        self.config = ClusteringConfig()
    
    def test_invalid_algorithm_handling(self):
        """Test handling of invalid algorithm specification."""
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_newsletter_items(20)
        
        with self.assertRaises(ValueError):
            result = orchestrator.cluster_items(
                items=items,
                algorithm="invalid_algorithm"
            )
    
    def test_malformed_input_handling(self):
        """Test handling of malformed input data."""
        orchestrator = ClusteringOrchestrator(self.config)
        
        # Test with items missing required fields
        malformed_items = [
            {"headline": "Test"},  # Missing short_description
            {"short_description": ""},  # Empty description
            {},  # Empty item
        ]
        
        # Should handle gracefully without crashing
        result = orchestrator.cluster_items(items=malformed_items)
        self.assertIsNotNone(result)
    
    def test_timeout_handling(self):
        """Test that processing timeout is handled correctly."""
        self.config.performance.max_processing_time_seconds = 0.1  # Very short timeout
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_performance_test_items(100)
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(items=items)
        
        # Should return timeout result
        # self.assertIn("error", result)
        # self.assertEqual(result["clustering_result"]["algorithm"], "timeout")
    
    def test_memory_limit_handling(self):
        """Test handling when memory limits are approached."""
        self.config.performance.max_memory_usage_gb = 0.001  # Very low limit
        orchestrator = ClusteringOrchestrator(self.config)
        items = TestDataGenerator.create_performance_test_items(50)
        
        with self.assertRaises(NotImplementedError):
            result = orchestrator.cluster_items(items=items)
        
        # Should complete but warn about memory usage
        # self.assertIn("warnings", result)


class TestAgentIntegration(unittest.TestCase):
    """Integration tests with the main agent.py workflow."""
    
    def setUp(self):
        """Set up agent integration test configuration."""
        self.config = ClusteringConfig()
    
    @unittest.skipIf(not CLUSTERING_AVAILABLE, "Clustering components not available")
    def test_agent_clustering_integration(self):
        """Test integration with main agent workflow."""
        # This test will verify that clustering integrates properly with agent.py
        # We'll mock the agent's newsletter processing workflow
        
        with patch('newsletter_agent_core.agent.ClusteringOrchestrator') as mock_orchestrator:
            mock_result = {
                "total_clusters": 5,
                "total_items": 50,
                "is_valid": True,
                "quality_score": 0.75
            }
            mock_orchestrator.return_value.cluster_items.return_value = mock_result
            
            # Import and test agent integration
            try:
                from newsletter_agent_core.agent import NewsletterAgent
                agent = NewsletterAgent()
                
                # Test that clustering is properly integrated
                # This should not raise an exception
                self.assertTrue(hasattr(agent, 'clustering_enabled'))
                
            except ImportError:
                self.skipTest("Agent module not available for integration testing")


if __name__ == "__main__":
    # Configure test runner
    unittest.main(
        verbosity=2,
        failfast=False,
        buffer=True,
        warnings='ignore'
    )